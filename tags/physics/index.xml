<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>physics on Asad Raza</title>
    <link>https://araza6.github.io/tags/physics/</link>
    <description>Recent content in physics on Asad Raza</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Â© Copyright notice</copyright>
    <lastBuildDate>Mon, 27 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://araza6.github.io/tags/physics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Differentiating exponentials of Hamiltonians</title>
      <link>https://araza6.github.io/posts/hamiltonian-differentiation/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://araza6.github.io/posts/hamiltonian-differentiation/</guid>
      <description>Let me start by claiming that quantum machine learning is basically unitary learning. Take circuit learning for example. We start with an initial state, which is almost always $| 0 \rangle$, apply a bunch of gates in each layer and continue to do so based on how much is our appetite for decoherence goes. Then we knot qubits up with some entanglement and finally measure on some or all qubits with respect to an operator.</description>
    </item>
    
    <item>
      <title>Interfacing JAX with QuTiP</title>
      <link>https://araza6.github.io/posts/qobj-jax-interface/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://araza6.github.io/posts/qobj-jax-interface/</guid>
      <description>As the month of June comes to an end, so does the first phase of Google Summer of Code (GSoC) 2020. In my previous post, I attempted to cover the theory behind auto-differentiation, but did not fully explain how am I going to use autodiff for my GSoC project. Here, I am going to discuss just that. I will explain what problems did I face to make QuTiP work with JAX, the famed auto-differentitation library, and what route did I take to solve the problem.</description>
    </item>
    
    <item>
      <title>Introduction to Autodifferentiation in Machine Learning</title>
      <link>https://araza6.github.io/posts/autodiff/autodiff/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://araza6.github.io/posts/autodiff/autodiff/</guid>
      <description>Disclaimer: This blog essentially summarizes this amazing review paper: Automatic Differentiation in Machine Learning: a Survey. All the tables and (fancy) images in the blog are taken from the paper. Invested readers should check out the full paper (linked).
Autodiff (short for Autodifferentiation) is extensively used in modern machine learning tasks. Libraries like Tensorflow, Pytorch, JAX and several others have made it easier to calculate the derivatives of arbitrary functions. In this blog, I will be going through the theory behind autodiff, explain what autodiff is NOT, and finally chain it up to machine learning (with a small code-snippet too).</description>
    </item>
    
    <item>
      <title>First Blog Post: GSoC 2020</title>
      <link>https://araza6.github.io/posts/welcome-gsoc/</link>
      <pubDate>Fri, 22 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://araza6.github.io/posts/welcome-gsoc/</guid>
      <description>Hello,
This is my first-ever blog post (as far as I remember really). I planned to start blogging in my freshman year, and it&amp;rsquo;s been almost three years since then. Planning fallacy, as Kahneman would say. Anyway, I should thank Google Summer of Code to finally make me do it (although technically so. Fret not, I will try to make these blogs as accessible as possible) as part of my upcoming summer project with NUMFOCUS&amp;rsquo;s QuTiP.</description>
    </item>
    
  </channel>
</rss>